"""
ref: https://chatgpt.com/g/g-cH94JC5NP-dspy-guide-v2024-2-7/c/66e7ba95-399c-8001-91a4-a1a26b8fbed5
paper that icl can predict if ft will work: https://arxiv.org/abs/2405.00200
"""
import dspy
from dspy.datasets import HotPotQA
from dspy.evaluate.evaluate import Evaluate

# Set up the language model (LM) using OpenAI GPT-3.5-turbo
turbo = dspy.OpenAI(model='gpt-3.5-turbo')

# Set up the retriever model (RM) using ColBERTv2 for document retrieval
colbertv2 = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')  # Example retriever endpoint

# Configure DSPy to use both LM and RM
dspy.settings.configure(lm=turbo, rm=colbertv2)

# Load the dataset (you can modify this to your specific math dataset if needed)
dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)

# Process the dataset to work with DSPy
trainset = [dspy.Example(question=x.question).with_inputs('question') for x in dataset.train]
devset = [dspy.Example(question=x.question).with_inputs('question') for x in dataset.dev]

# 1. Define the signature for generating math problems with solutions for retrieved contexts
class MathProblemGeneration(dspy.Signature):
    """Generate math question-answer pairs for a list of retrieved contexts."""
    contexts = dspy.InputField(desc="Retrieved math contexts to generate problems from")
    question_answer_pairs = dspy.OutputField(desc="Generated question-answer pairs for each context")

# 2. Define ICL Module that takes multiple few-shot examples (problem-solution pairs)
class ICLMathModule(dspy.Signature):
    """Use ICL with multiple math problem-solution pairs to generate an answer."""
    examples = dspy.InputField(desc="List of problem-solution pairs for few-shot ICL")
    question = dspy.InputField(desc="New math question to solve")
    answer = dspy.OutputField(desc="Answer generated by ICL")

# 3. DSPy module for generating synthetic math problems and using ICL with retrieval
class MathPipelineWithRM(dspy.Module):
    def __init__(self):
        super().__init__()

        # Step 1: Retrieve relevant contexts using the ColBERTv2 model
        self.retrieve = dspy.Retrieve(k=5)  # Retrieve top-5 relevant contexts
        
        # Step 2: Generate synthetic math problems with solutions from retrieved contexts
        self.generate_math_problems = dspy.ChainOfThought(MathProblemGeneration)
        
        # Step 3: Use ICL with the generated math problem-solution pairs to answer a new question
        self.answer_math_icl = dspy.ChainOfThought(ICLMathModule)

    def forward(self, question):
        # Step 1: Retrieve relevant contexts from the retriever model
        retrieved_contexts = self.retrieve(question).passages
        
        # Step 2: Generate synthetic math problem-solution pairs from the retrieved contexts
        synthetic_result = self.generate_math_problems(contexts=retrieved_contexts)
        generated_qa_pairs = synthetic_result.question_answer_pairs
        
        # Extract the first 5 examples (or as many as generated) for few-shot ICL
        icl_examples = generated_qa_pairs[:5]
        
        # Step 3: Use ICL to answer a new question based on the examples
        icl_result = self.answer_math_icl(examples=icl_examples, question=question)
        icl_answer = icl_result.answer

        return dspy.Prediction(
            retrieved_contexts=retrieved_contexts,
            synthetic_qa_pairs=generated_qa_pairs,
            answer=icl_answer
        )

# 4. Teleprompter setup with BootstrapFewShot
from dspy.teleprompt import BootstrapFewShot

# Validation function: check if predicted answer matches expected answer (exact match metric)
def validate_math_answer(example, pred, trace=None):
    # Compare the generated answer with the expected answer using exact match
    return dspy.evaluate.answer_exact_match(example, pred)

# Teleprompter for optimization
teleprompter = BootstrapFewShot(metric=validate_math_answer)

# Compile the math generation and ICL pipeline with retrieval using the teleprompter
compiled_math_pipeline_rm = teleprompter.compile(MathPipelineWithRM(), trainset=trainset)

# 5. Set up the evaluation function for the model
evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=False, display_table=5)

# Sample math question to be solved with ICL
sample_math_question = "What is the sum of the angles in a triangle?"

# Run the compiled pipeline with the sample question
pred = compiled_math_pipeline_rm(sample_math_question)

# Print the results: retrieved contexts, synthetic question-answer pairs, and ICL-generated answer
print(f"Question: {sample_math_question}")
print(f"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.retrieved_contexts]}")
print(f"Synthetic Question-Answer Pairs: {pred.synthetic_qa_pairs}")
print(f"ICL Answer: {pred.answer}")

# 6. Evaluate the pipeline on the dev set using the exact match metric
metric = dspy.evaluate.answer_exact_match
evaluation_result = evaluate_on_hotpotqa(compiled_math_pipeline_rm, metric=metric)

# Print the evaluation results
print("Evaluation result:", evaluation_result)

# Optionally, print out a few final examples from the dev set to analyze
for example in devset[:5]:
    pred = compiled_math_pipeline_rm(example['question'])
    print(f"Question: {example['question']}")
    print(f"Retrieved Context: {pred.retrieved_contexts}")
    print(f"Synthetic Question-Answer Pairs: {pred.synthetic_qa_pairs}")
    print(f"ICL Answer: {pred.answer}")

# Save the compiled pipeline
compiled_math_pipeline_rm.save('compiled_math_pipeline_with_rm.dspy')

# Extract and print the final compiled prompts for each module
for trace in teleprompter.trace(compiled_math_pipeline_rm):
    print(f"Module: {trace.module}")
    print(f"Compiled Prompt:\n{trace.prompt}\n")
