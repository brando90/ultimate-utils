import os
import dspy
from dspy.datasets import HotPotQA
from dspy.evaluate.evaluate import Evaluate

# # Set up the language model (LM) using OpenAI GPT-3.5-turbo
# turbo = dspy.OpenAI(model='gpt-3.5-turbo')
# dspy.settings.configure(lm=turbo)
# Configure DSPy to use the local HF model as the LM.
# export CUDA_VISIBLE_DEVICES=5
os.environ['CUDA_VISIBLE_DEVICES'] = '5'
model_name = "meta-llama/Llama-2-7b-hf"  # Adjust model name as necessary
# model_name = "gpt2"  # Adjust model name as necessary
hf_lm = dspy.HFModel(model=model_name)
dspy.settings.configure(lm=hf_lm) 
    

# Load the dataset (you can modify this to your specific math dataset if needed)
dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)

# Define a hardcoded set of math contexts (facts) for all examples
math_contexts = [
    "The sum of the angles in a triangle is 180 degrees.",
    "The Pythagorean theorem states that a^2 + b^2 = c^2 in a right triangle.",
    "The area of a circle is πr^2 where r is the radius.",
    "The circumference of a circle is 2πr where r is the radius.",
    "A prime number is a number greater than 1 that has no divisors other than 1 and itself.",
    "The quadratic formula is x = (-b ± √(b²-4ac)) / 2a.",
    "The sum of an arithmetic series is given by S = n/2 * (a + l), where a is the first term, l is the last term, and n is the number of terms.",
    "Euler’s formula is e^(iθ) = cos(θ) + i*sin(θ).",
    "The binomial theorem is (a + b)^n = Σ (n choose k) * a^(n-k) * b^k.",
    "The area of a trapezoid is (1/2) * (b1 + b2) * h, where b1 and b2 are the lengths of the two parallel sides and h is the height."
]

# Process the dataset to work with DSPy
# trainset = [x.with_inputs('question') for x in dataset.train]
# devset = [x.with_inputs('question') for x in dataset.dev]
# Process the dataset to work with DSPy, adding the hardcoded contexts as an input
trainset = [dspy.Example(question=x.question, contexts=math_contexts, answer=x.answer).with_inputs('question', 'contexts') for x in dataset.train]
devset = [dspy.Example(question=x.question, contexts=math_contexts, answer=x.answer).with_inputs('question', 'contexts') for x in dataset.dev]

# 1. Define the signature for generating math problems with solutions for each context
class MathProblemGeneration(dspy.Signature):
    """Generate math question-answer pairs for a list of contexts."""
    contexts = dspy.InputField(type=list[str], desc="List of math contexts to generate problems from")
    question_answer_pairs = dspy.OutputField(type=list[str], desc="Generated question-answer pairs for each context")

# 2. Define ICL Module that takes multiple few-shot examples (problem-solution pairs)
class ICLMathModule(dspy.Signature):
    """Use ICL with multiple math problem-solution pairs to generate an answer."""
    # context = dspy.InputField(desc="Math context for ICL")
    examples = dspy.InputField(type=list[str], desc="List of problem-solution pairs for few-shot ICL")
    question = dspy.InputField(type=str, desc="New math question to solve")
    answer = dspy.OutputField(type=str, desc="Answer generated by ICL")

# 3. DSPy module for generating synthetic math problems and using ICL
class MathPipeline(dspy.Module):
    def __init__(self):
        super().__init__()

        # Phase 1: Generate synthetic math problems with solutions from multiple contexts
        self.generate_math_problems = dspy.ChainOfThought(MathProblemGeneration)
        
        # Phase 2: Use ICL with the generated math problem-solution pairs to answer a new question
        self.answer_math_icl = dspy.ChainOfThought(ICLMathModule)

    def forward(self, contexts: list[str], question: str):
        _contexts: str = '\n'.join(contexts)
        # Step 1: Generate synthetic math problem-solution pairs from the list of contexts
        synthetic_result = self.generate_math_problems(contexts=_contexts)
        generated_qa_pairs = synthetic_result.question_answer_pairs
        
        # Extract the first 5 examples (or as many as generated) for few-shot ICL
        icl_examples = generated_qa_pairs[:5]
        
        # Step 2: Use ICL to answer a new question based on the examples
        icl_result = self.answer_math_icl(examples=icl_examples, question=question)
        icl_answer = icl_result.answer

        return dspy.Prediction(
            synthetic_qa_pairs=generated_qa_pairs, # we don't need to return it but I will leave it there...?
            answer=icl_answer
        )

# 4. Teleprompter setup with BootstrapFewShot
from dspy.teleprompt import BootstrapFewShot

# Validation function: check if predicted answer matches expected answer (exact match metric)
def validate_math_answer(example, pred, trace=None):
    # Compare the generated answer with the expected answer using exact match
    return dspy.evaluate.answer_exact_match(example, pred)

# Teleprompter for optimization
teleprompter = BootstrapFewShot(metric=validate_math_answer)

# Compile the math generation and ICL pipeline using the teleprompter
compiled_math_pipeline = teleprompter.compile(MathPipeline(), trainset=trainset)

# 5. Set up the evaluation function for the model
evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=False, display_table=5)

# List of math contexts for generating synthetic problems
math_contexts = [
    "Properties of triangles and circles in Euclidean geometry.",
    "Basic algebra with quadratic equations.",
    "Number theory: prime numbers, divisibility rules.",
    "Combinatorics: counting principles and permutations.",
    "Trigonometry: sine and cosine rules in triangles."
]

# Sample new math question to be solved with ICL
sample_math_question = "What is the sum of angles in a triangle?"

# Run the compiled pipeline with the sample math contexts and question
pred = compiled_math_pipeline(math_contexts, sample_math_question)

# Print the results: synthetic question-answer pairs and ICL-generated answer
print(f"Math Contexts: {math_contexts}")
print(f"Synthetic Question-Answer Pairs: {pred.synthetic_qa_pairs}")
print(f"ICL Answer: {pred.icl_answer}")

# 6. Evaluate the pipeline on the dev set using the exact match metric
metric = dspy.evaluate.answer_exact_match
evaluation_result = evaluate_on_hotpotqa(compiled_math_pipeline, metric=metric)

# Print the evaluation results
print("Evaluation result:", evaluation_result)

# Optionally, print out a few final examples from the dev set to analyze
for example in devset[:5]:
    pred = compiled_math_pipeline(example['context'], example['question'])
    print(f"Math Context: {example['context']}")
    print(f"Synthetic Question-Answer Pairs: {pred.synthetic_qa_pairs}")
    print(f"ICL Answer: {pred.icl_answer}")