# vllm with dspy

```bash
# bellow failed to install vllm with uutils first installing it with default setup.py then 
pip install --upgrade pip
pip install torch==2.2.1
pip install vllm==0.4.1

# worked on skampere1
pip install --upgrade pip
pip uninstall torchvision vllm vllm-flash-attn flash-attn xformers
pip install torch==2.2.1 vllm==0.4.1 
# fails
# pip install flash-attn==2.6.3
```

```bash
export CUDA_VISIBLE_DEVICES=5
python -m vllm.entrypoints.api_server --model meta-llama/Llama-2-7b-hf --port 8080

python ~/ultimate-utils/py_src/uutils/dspy_uu/examples/full_toy_vllm_local_mdl.py
```
ask tom: https://github.com/stanfordnlp/dspy/issues/1002
ref: gpt and me https://chatgpt.com/c/66e4d31b-20e0-8001-a10f-7835d3b17182


# HF run

```bash
export CUDA_VISIBLE_DEVICES=5

```