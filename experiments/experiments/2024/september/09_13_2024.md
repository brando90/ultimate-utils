# vllm with dspy

```bash
# this installed flash but vllm didn't say in it's output it was using it
pip install --upgrade pip
pip install torch==2.4.0
pip install vllm==0.5.4
pip install flash-attn==2.6.3

# def does not work for mac
# # -- ref: https://github.com/vllm-project/vllm/issues/2747 
# pip install torch==2.2.1
# pip install vllm==0.4.1
# 'torch==2.2.1',
# 'vllm==0.4.1', 

# bellow failed to install vllm with uutils first installing it with default setup.py then 
pip install --upgrade pip
pip install torch==2.2.1
pip install vllm==0.4.1

# worked on skampere1
pip install --upgrade pip
pip uninstall torchvision vllm vllm-flash-attn flash-attn xformers
pip install torch==2.2.1 vllm==0.4.1 
# fails
# pip install flash-attn==2.6.3
```

```bash
export CUDA_VISIBLE_DEVICES=5
python -m vllm.entrypoints.api_server --model meta-llama/Llama-2-7b-hf --port 8080

python ~/ultimate-utils/py_src/uutils/dspy_uu/examples/full_toy_vllm_local_mdl.py

# fails to run server!
# python -m vllm.entrypoints.api_server --model meta-llama/Meta-Llama-3.1-8B-Instruct --port 8080
```
ask tom: https://github.com/stanfordnlp/dspy/issues/1002
ref: gpt and me https://chatgpt.com/c/66e4d31b-20e0-8001-a10f-7835d3b17182
