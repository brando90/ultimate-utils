# dilara's msg
```md
Dilara Soylu
  10:41 PM
Sg! For any option you pick (TGI, SGLang, VLLM), the first step is to start the respective server that will host the LM you are interested in. Then, you will use the respective “client” in DSPy to access this.
I can speak about how to go about these for TGI. Here is an example command for starting a TGI server:
export HUGGING_FACE_HUB_TOKEN=<FILL_TOKEN>; docker run -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN --gpus “device=0" --shm-size 1g -p 1880:80 -v <FILL_PATH>:/data ghcr.io/huggingface/text-generation-inference:1.3 --model-id mistralai/Mistral-7B-Instruct-v0.2 --num-shard 1 --max-best-of 100 --max-input-length 8191 --max-total-tokens 8192 --max-batch-prefill-tokens 8192
Sharing more details as to the important settings in the thread.

1 reply
18 hours agoView thread

Dilara Soylu
  Yesterday at 10:41 PM
The particular model I’m using here (mistralai/Mistral-7B-Instruct-v0.2) requires a permission to access, which is the reason why we are providing an HF Token (<FILL_TOKEN>, you’d exclude the brackets when typing in your token) The model you want to use may not require it. You can substitute any HF model instead of this, including any private models you may have under your account.
The device=0 part is telling TGI which GPUs to use. If you want to target multiple gpus, you can add a comma separated list device=5,6 etc. This would set up TGI on the GPUs numbered 5 and 6.
1880:80 is specifying the port that the server will be started at. You can modify 1880 as you wish!
The <FILL_PATH> portion is referring to the path that you want your TGI docker instance to mount. For me, this is a path of the form /fast-scr-space/username/docker/tgi. It can be any place you have access to. Note that this directory might be owned by a root user (docker that I’m using is root only, so any file created as part of the docker run will also be owned by root) This just means that your regular user may not be able to modify this directory later on. This is not too important, unless you want to save disk space. I use the exact same mounting point to limit the number of directories created by the root user.
I’m using a particular version of TGI, to prevent myself from debugging various setup/env related errors. You can go for the latest version by dropping the :1.3
If you want to run a local HF model, you should first make a local copy of your model under the path above (e.g. /fast-scr-space/username/docker/tgi) Say you created a subpath here, say local/model_1880 You can then replace the model id with /data/local/model_1880 Note that the data here is the same as the data in <FILL_PATH>:/data (if you change the name here, you should also change the reference in the model id) I’m running multiple models at once, each using a different port number. To prevent any confusion, I’m including the port number for the model in the model id path I copy the weights to.


Dilara Soylu
  10:51 PM
Once you run this command, you need to wait a bit (30 seconds or so for my setting), and see the server running. Once the server is up and running, you can use the TGI client in DSPy.
dspy.HFClientTGI(model=model_id, port=port, url=url, max_tokens=TGI_MAX_TOKENS, stop=stop, temperature=temperature)
The model parameter here doesn’t actually do anything* You can just pass the HF id of your model (or the local path if your model is local) HFClientTGI cares about the port and url you specify to find out where to connect to. The max_tokens, stop and temperature are parameters that are directly sent to the TGI server and they determine the inference settings as you might guess. Stop here is a stopping string you want to specify. Re-iterating that the model id isn’t sent to the TGI server when you make calls.
* The model parameter (or any other parameter you pass to the constructor of HFClientTGI / other LMs in DSPy, is important for caching. The next time you make calls to this particular model at this particular url/port, you can make use of caching if you pass the same model id.
** Say you want to make calls to the same TGI instance, but don’t want to re-use cache. HFClientTGI / any other DSPy LM accepts arbitrary keywords. For example, you can create a new keyword cache_id and pass any (immutable) value you want to this, and control caching. DSPy / TGI doesn’t do anything with these extra parameters, but they are saved as part of the cache.
*** Let me know if you want to make caching independent of the port/url — can suggest other small changes.
**** DSPy 2.5 (releasing soon) is introducing many changes wrt to the LM interface. HFClientTGI  (or any other local client) may not be supported immediately. They will be replaced with better alternatives, but this may not be immediate. You may want to stick to the version you have to keep using these local models.
```

# vllm with dspy

Install vllm & try running dspy vllm server
```bash
# - Install vllm
# FAILED: bellow failed to install vllm with uutils first installing it with default setup.py then 
# pip install --upgrade pip
# pip install torch==2.2.1
# pip install vllm==0.4.1
# - Installed vllm on skampere1
pip install --upgrade pip
pip uninstall torchvision vllm vllm-flash-attn flash-attn xformers
pip install torch==2.2.1 vllm==0.4.1 
# fails install
# pip install flash-attn==2.6.3

# -- DSPy with vLLM 
# - Run vllm server
export CUDA_VISIBLE_DEVICES=1
# python -m vllm.entrypoints.api_server --model meta-llama/Llama-2-7b-hf --port 8080
python -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --port 8080

# - Run dspy vllm-client
python ~/ultimate-utils/py_src/uutils/dspy_uu/examples/full_toy_vllm_local_mdl.py
```

ask tom: https://github.com/stanfordnlp/dspy/issues/1002
ref: gpt and me https://chatgpt.com/c/66e4d31b-20e0-8001-a10f-7835d3b17182

```bash
# -- Run HFTGI as Dilara suggested
export CUDA_VISIBLE_DEVICES=1
export HUGGING_FACE_HUB_TOKEN=$(cat ~/keys/brandos_hf_token.txt) 
# cat brandos_hf_token.txt
# export HUGGING_FACE_HUB_TOKEN=...
# echo $HUGGING_FACE_HUB_TOKEN
export VOLUME_PATH=~/data
# docker run -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN --gpus "device=0" --shm-size 1g -p 1880:80 -v $VOLUME_PATH:/data ghcr.io/huggingface/text-generation-inference:1.3 --model-id mistralai/Mistral-7B-Instruct-v0.2 --num-shard 1 --max-best-of 100 --max-input-length 8191 --max-total-tokens 8192 --max-batch-prefill-tokens 8192
# with trust remote code
docker run -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN --gpus "device=0" --shm-size 1g -p 1880 -v $VOLUME_PATH:/data ghcr.io/huggingface/text-generation-inference:1.3 --model-id mistralai/Mistral-7B-Instruct-v0.2 --num-shard 1 --max-best-of 100 --max-input-length 8191 --max-total-tokens 8192 --max-batch-prefill-tokens 8192 --trust-remote-code

# I’m using a particular version of TGI, to prevent myself from debugging various setup/env related errors. You can go for the latest version by dropping the :1.3
export CUDA_VISIBLE_DEVICES=1
export HUGGING_FACE_HUB_TOKEN=$(cat ~/keys/brandos_hf_token.txt) 
cat ~/keys/brandos_hf_token.txt

# echo $HUGGING_FACE_HUB_TOKEN
export VOLUME_PATH=~/data
docker run -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN --gpus "device=0" --shm-size 1g -p 1880 -v $VOLUME_PATH:/data ghcr.io/huggingface/text-generation-inference --model-id mistralai/Mistral-7B-Instruct-v0.2 --num-shard 1 --max-best-of 100 --max-input-length 8191 --max-total-tokens 8192 --max-batch-prefill-tokens 8192


# -- Try DSPY script
python ~/ultimate-utils/py_src/uutils/dspy_uu/examples/full_toy_vllm_local_mdl.py
# used
```
good attempt: https://chatgpt.com/c/66ea1a8f-9f10-8001-9489-4a2a04e21faa
